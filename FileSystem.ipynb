{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T17:15:53.730598Z",
     "start_time": "2020-07-14T17:15:53.592590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Users\\feelus\\Repos\\hash_sync\n"
     ]
    }
   ],
   "source": [
    "from FileSystem import *\n",
    "import os\n",
    "home_dir = os.getcwd()\n",
    "print(home_dir)\n",
    "import datetime as dt\n",
    "from datetime import *\n",
    "curtz = datetime.now().astimezone().tzinfo\n",
    "tform = '%Y-%m-%d %H-%M-%S%z'\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T17:15:53.763600Z",
     "start_time": "2020-07-14T17:15:53.733599Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_date_file(prefix,postfix,ls):\n",
    "    return [s[len(prefix):-len(postfix)] for s in ls if \\\n",
    "              s.startswith(prefix) and s.endswith(postfix)]\n",
    "\n",
    "\n",
    "from copy import *\n",
    "def last_diff_dir(prefix,exclude_dirs={},global_log=None):\n",
    "    prefix_ = normalize_path(prefix+os.sep)\n",
    "    emergency_dump = True\n",
    "    newtime_s = None\n",
    "    snapshot_json = None\n",
    "    oldtime_s = None\n",
    "    snapshot_bak = None\n",
    "    root = None\n",
    "    oldroot = None\n",
    "    errors = None\n",
    "    olderrors = None\n",
    "    patch = None\n",
    "    try:\n",
    "        os.chdir(prefix_)\n",
    "        loclog = open('.files/log.txt','a')\n",
    "    except Exception as e:\n",
    "        if global_log:\n",
    "            with open(global_log,'a') as gl:\n",
    "                gl.write(prefix+'\\t'+'device unavailable\\n')\n",
    "            return\n",
    "        else:\n",
    "            raise e\n",
    "    globlog = open(global_log,'a') if global_log else None\n",
    "    def log(s):\n",
    "        s = str(s)[:200]\n",
    "        print(s)\n",
    "        loclog.write(s+'\\n')\n",
    "        loclog.flush()\n",
    "        if globlog:\n",
    "            globlog.write(prefix+'\\t'+s+'\\n')\n",
    "            globlog.flush()\n",
    "    def error(e):\n",
    "        log(e)\n",
    "        if not globlog:\n",
    "            raise e\n",
    "    \n",
    "    try:\n",
    "        newtime = datetime.now(curtz)\n",
    "        newtime_s = newtime.strftime(tform)\n",
    "        log(prefix_+'\\t'+newtime_s)\n",
    "        snapshot_json = '.files/last_snapshot '+newtime_s+'.json'\n",
    "        def find_date_file(prefix,postfix,ls):\n",
    "            return [s[len(prefix):-len(postfix)] for s in ls if \\\n",
    "                      s.startswith(prefix) and s.endswith(postfix)]\n",
    "        if '.files' in os.listdir('.'):\n",
    "            ls = os.listdir('.files\\\\')\n",
    "            oldtime_list = find_date_file('last_snapshot ','.bak',ls)\n",
    "            assert len(oldtime_list)<=1\n",
    "            if len(oldtime_list)==1:\n",
    "                oldtime_s = oldtime_list[0]\n",
    "                snapshot_bak = '.files/last_snapshot '+oldtime_s+'.bak'\n",
    "\n",
    "                oldroot,olderrors = load_snapshot(snapshot_bak) # <----\n",
    "                newtime_list = find_date_file('last_snapshot ','.json',ls)\n",
    "                assert len(newtime_list)<=1\n",
    "                if len(newtime_list)==1:\n",
    "                    print('fast recovery')\n",
    "                    # сканируем-пересчитываем на основе .json\n",
    "                    # затем удаляем его и сохраняем новый\n",
    "                    newtime_s = newtime_list[0]\n",
    "                    snapshot_json = '.files/last_snapshot '+newtime_s+'.json'\n",
    "\n",
    "                    root,errors = load_snapshot(snapshot_json) # <----\n",
    "                    #root,errors = scan(prefix_,exclude_dirs)\n",
    "                    #calc_hashes(root,errors,oldnewroot,prefix)\n",
    "\n",
    "                    #os.remove(oldsnapshot_json)\n",
    "                    #dump_snapshot(root,errors,snapshot_json) # ---->\n",
    "                else:\n",
    "                    print('recovery')\n",
    "                    # сканируем-пересчитываем на основе .bak\n",
    "                    # затем сохраняем новый\n",
    "                    root,errors = scan(prefix_,exclude_dirs)\n",
    "                    calc_hashes(root,errors,oldroot,prefix)\n",
    "\n",
    "                    dump_snapshot(root,errors,snapshot_json) # ---->\n",
    "            else:\n",
    "                print('simple update')\n",
    "                # сканируем-пересчитываем на основе json\n",
    "                # затем переименовываем его в .bak и сохраняем новый\n",
    "                oldtime_list = find_date_file('last_snapshot ','.json',ls)\n",
    "                assert len(oldtime_list)==1\n",
    "                oldtime_s = oldtime_list[0]\n",
    "                snapshot_bak = '.files/last_snapshot '+oldtime_s+'.bak'\n",
    "\n",
    "                olderrors,oldroot = load_snapshot('.files/last_snapshot '+oldtime_s+'.json') # <----\n",
    "                root,errors = scan(prefix_,exclude_dirs)\n",
    "                calc_hashes(root,errors,oldroot,prefix)\n",
    "\n",
    "                os.rename('.files/last_snapshot '+oldtime_s+'.json',snapshot_bak)\n",
    "                dump_snapshot(errors,root,snapshot_json) # ---->\n",
    "        else:\n",
    "            print('create all')\n",
    "            # записываем пустое дерево в .bak (дата на минуту раньше текущей)\n",
    "            # сканируем-пересчитываем на основе .bak\n",
    "            # затем сохраняем новый\n",
    "            os.mkdir('.files')\n",
    "            oldtime_s = (newtime - timedelta(minutes=1)).strftime(tform)\n",
    "            snapshot_bak = '.files/last_snapshot '+oldtime_s+'.bak'\n",
    "            oldroot = {}\n",
    "            root,errors = scan(prefix_,exclude_dirs)\n",
    "            calc_hashes(root,errors,oldroot,prefix)\n",
    "\n",
    "            dump_snapshot({},oldroot,snapshot_bak) # ---->\n",
    "            dump_snapshot(errors,root,snapshot_json)\n",
    "    except BaseException as e:\n",
    "        error(Exception('scan:',e));        return\n",
    "\n",
    "    try:\n",
    "        oldroot_d = oldroot#{k:v for k,v in oldroot.items() if k!='__scan_errors__'}\n",
    "        root_d = root#{k:v for k,v in root.items() if k!='__scan_errors__'}\n",
    "\n",
    "        patch = {\n",
    "            'errors':path_patch_compress(*path_diff(olderrors,errors)),\n",
    "            'root':hash_patch_compress(*hash_diff(oldroot_d,root_d))\n",
    "        }\n",
    "\n",
    "        newoldroot = hash_back_patch(root_d,*hash_patch_uncompress(patch['root']))\n",
    "        assert oldroot_d==newoldroot\n",
    "\n",
    "        newerrors = path_patch(olderrors,*path_patch_uncompress(patch['errors']))\n",
    "        assert errors==newerrors\n",
    "    except BaseException as e:\n",
    "        if emergency_dump:\n",
    "            print('exception catched, writing \"exception_dump.json\"')\n",
    "            with open('.files/exception_dump.json','w') as file:\n",
    "                json.dump({'olderrors':olderrors,'errors':errors,'oldroot':oldroot,\n",
    "                           'root':root},      file)\n",
    "        error(Exception('diff:',e));        return\n",
    "        \n",
    "    try:\n",
    "        myjson_dump(patch,'.files/patch '+oldtime_s+' to '+newtime_s+'.json')\n",
    "        if find_date_file('first_snapshot ','.json',ls):\n",
    "            os.remove(snapshot_bak)\n",
    "        else:\n",
    "            os.rename(snapshot_bak,'.files/first_snapshot '+oldtime_s+'.json',)\n",
    "    except BaseException as e:\n",
    "        error(Exception('scan:',e));        return\n",
    "        \n",
    "    loclog.close()\n",
    "    if globlog: globlog.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T11:07:48.677723Z",
     "start_time": "2020-07-14T11:07:48.610719Z"
    }
   },
   "outputs": [],
   "source": [
    "global_log = r'D:\\Users\\feelus\\Desktop\\scan-diff.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T11:15:46.512053Z",
     "start_time": "2020-07-14T11:07:48.679723Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== D:\\ ===\n",
      "=== D:\\ ===\n",
      "simple update\n",
      "149 GB scanned - completed\n",
      "149 GB calculated, 0 GB calculated (99% cached)- completed\n",
      "start writing\n",
      "writed .files/last_snapshot 2020-07-14 14-07-48+0300.json\n",
      "--- ITERATION ---\n",
      "--- ITERATION ---\n",
      "--- FINAL ITERATION ---\n",
      "start writing\n",
      "writed .files/patch 2020-07-13 22-29-28+0300 to 2020-07-14 14-07-48+0300.json\n"
     ]
    }
   ],
   "source": [
    "last_diff_dir('D:',{r'D:\\Users\\feelus\\YandexDisk',r'D:\\$RECYCLE.BIN'},global_log)\n",
    "# executed in 8m 51s, finished 22:38:19 2020-07-13\n",
    "# executed in 7m 58s, finished 14:15:46 2020-07-14 149 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T11:25:51.777672Z",
     "start_time": "2020-07-14T11:15:46.516053Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== C:\\ ===\n",
      "=== C:\\ ===\n",
      "simple update\n",
      "142 GB scanned - completed\n",
      "142 GB calculated, 0 GB calculated (99% cached)- completed\n",
      "start writing\n",
      "writed .files/last_snapshot 2020-07-14 14-15-46+0300.json\n",
      "--- ITERATION ---\n",
      "--- FINAL ITERATION ---\n",
      "start writing\n",
      "writed .files/patch 2020-07-14 13-10-54+0300 to 2020-07-14 14-15-46+0300.json\n"
     ]
    }
   ],
   "source": [
    "#patch 2020-05-31 11-40-04+0300 to 2020-07-02 23-29-48+0300.json\n",
    "# Program Files/Mozilla Firefox/chrome.manifest\n",
    "#  файлы размера 0 содержится и в moved и в new/old\n",
    "last_diff_dir('C:',{r'C:\\$RECYCLE.BIN'},global_log)\n",
    "# executed in 11m 20s, finished 11:55:56 2020-07-14 \n",
    "# executed in 10m 5s, finished 14:25:51 2020-07-14 142 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T11:26:56.249360Z",
     "start_time": "2020-07-14T11:25:51.785673Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== D:\\Users\\feelus\\YandexDisk\\ ===\n",
      "=== D:\\Users\\feelus\\YandexDisk\\ ===\n",
      "simple update\n",
      "207 GB scanned - completed\n",
      "207 GB calculated, 0 GB calculated (99% cached)- completed\n",
      "start writing\n",
      "writed .files/last_snapshot 2020-07-14 14-25-51+0300.json\n",
      "--- ITERATION ---\n",
      "--- FINAL ITERATION ---\n",
      "start writing\n",
      "writed .files/patch 2020-07-14 14-02-26+0300 to 2020-07-14 14-25-51+0300.json\n"
     ]
    }
   ],
   "source": [
    "last_diff_dir(r'D:\\Users\\feelus\\YandexDisk',{},global_log)\n",
    "# executed in 1m 4.46s, finished 14:26:56 2020-07-14 207 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T11:45:31.372434Z",
     "start_time": "2020-07-14T11:45:31.366433Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_diff_dir('H:',{r'H:\\$RECYCLE.BIN'},global_log)\n",
    "# executed in 1.56s, finished 14:26:57 2020-07-14 343 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T11:45:32.463496Z",
     "start_time": "2020-07-14T11:45:32.455495Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_diff_dir('I:',{r'I:\\$RECYCLE.BIN'},global_log)\n",
    "# executed in 12m 44s, finished 14:39:42 2020-07-14 330 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T11:39:42.087163Z",
     "start_time": "2020-07-14T11:39:42.082163Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.system(\"shutdown /s /t 1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T11:39:43.044218Z",
     "start_time": "2020-07-14T11:39:42.090164Z"
    }
   },
   "outputs": [
    {
     "ename": "BaseException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBaseException\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-409fa8c7e39a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mraise\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mBaseException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise BaseException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
